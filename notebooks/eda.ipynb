{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# EDA: Twitter US Airline Sentiment\n",
        "\n",
        "This notebook summarizes data exploration steps to justify preprocessing and model choices for the sentiment-aware chatbot.\n",
        "\n",
        "Authors: Paula Llanos López, Samuel Rivero, Sara López Marín (EAFIT)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "\n",
        "pd.set_option('display.max_colwidth', 200)\n",
        "\n",
        "DATA_PATH = '../data/Tweets.csv'\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "df = df[['airline_sentiment','text']].dropna()\n",
        "df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Class distribution\n",
        "ax = df['airline_sentiment'].value_counts(normalize=True).mul(100).plot(kind='bar', color=['tab:red','tab:blue','tab:green'])\n",
        "plt.title('Class distribution (%)')\n",
        "plt.ylabel('%')\n",
        "plt.xlabel('airline_sentiment')\n",
        "plt.show()\n",
        "\n",
        "df['len'] = df['text'].astype(str).str.len()\n",
        "df['tokens'] = df['text'].astype(str).str.split().str.len()\n",
        "fig, axes = plt.subplots(1,2, figsize=(10,4))\n",
        "sns.boxplot(data=df, x='airline_sentiment', y='len', ax=axes[0])\n",
        "sns.boxplot(data=df, x='airline_sentiment', y='tokens', ax=axes[1])\n",
        "axes[0].set_title('Char length by class')\n",
        "axes[1].set_title('Token count by class')\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Simple text cleaning preview\n",
        "url_re = re.compile(r'http\\S+')\n",
        "mention_re = re.compile(r'@\\w+')\n",
        "hash_re = re.compile(r'#\\w+')\n",
        "newline_re = re.compile(r'[\\r\\n]+')\n",
        "space_re = re.compile(r'\\s+')\n",
        "\n",
        "def clean_text(s):\n",
        "    s = str(s).lower()\n",
        "    s = url_re.sub('', s)\n",
        "    s = mention_re.sub('', s)\n",
        "    s = hash_re.sub('', s)\n",
        "    s = newline_re.sub(' ', s)\n",
        "    s = space_re.sub(' ', s).strip()\n",
        "    return s\n",
        "\n",
        "sample = df.sample(5, random_state=1)\n",
        "sample.assign(cleaned=sample['text'].map(clean_text))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "- The dataset is imbalanced (negative dominates). We will use macro F1 and class_weight='balanced'.\n",
        "- Cleaning keeps negations and removes URLs, mentions, and hashtags.\n",
        "- TF–IDF with n-grams (1,2) should capture short phrases relevant for sentiment.\n",
        "- Train/test split is stratified before vectorization to avoid leakage.\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
